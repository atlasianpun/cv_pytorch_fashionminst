{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-px9usRYnJQz",
        "outputId": "f7fc4600-903d-43a2-e662-1eed1d706f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.3.1\n",
            "Uninstalling torch-2.3.1:\n",
            "  Successfully uninstalled torch-2.3.1\n",
            "Found existing installation: torchaudio 2.3.1\n",
            "Uninstalling torchaudio-2.3.1:\n",
            "  Successfully uninstalled torchaudio-2.3.1\n",
            "Found existing installation: torchvision 0.18.1\n",
            "Uninstalling torchvision-0.18.1:\n",
            "  Successfully uninstalled torchvision-0.18.1\n",
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "Found existing installation: torchdata 0.7.1\n",
            "Uninstalling torchdata-0.7.1:\n",
            "  Successfully uninstalled torchdata-0.7.1\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "Collecting torchtext\n",
            "  Using cached torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "Collecting torchdata\n",
            "  Using cached torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchtext, torchdata, torchaudio\n",
            "Successfully installed torch-2.3.1 torchaudio-2.3.1 torchdata-0.7.1 torchtext-0.18.0 torchvision-0.18.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "XX85EEFsnMPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "ZGAe452TpTVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "EssV0j-szbF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce404075-3815-429e-df2d-4d49d740a0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "P8Ocb-O-0r8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fe7d31-aa1b-4cb2-fb68-5607a2e32f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Get CPU, GPU, or MPS device for training\n",
        "device = (\n",
        "    \"cuda\"                                    # Check if CUDA (GPU) is available\n",
        "    if torch.cuda.is_available()               # If CUDA is available, use it\n",
        "    else \"mps\"                                 # Check if CUDA Managed MPS is available\n",
        "    if torch.backends.mps.is_available()       # If MPS is available, use it\n",
        "    else \"cpu\"                                 # Otherwise, use CPU\n",
        ")\n",
        "print(f\"Using {device} device\")                # Print the device being used\n",
        "\n",
        "# Define the neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()                     # Initialize the base class (nn.Module)\n",
        "\n",
        "        # Flatten layer to convert 2D image data (28x28) to 1D tensor (784)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Sequential stack of linear and ReLU layers\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),             # 1st fully connected layer: 784 input features, 512 output features\n",
        "            nn.ReLU(),                         # ReLU activation function\n",
        "            nn.Linear(512, 512),               # 2nd fully connected layer: 512 input features, 512 output features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),                         # ReLU activation function\n",
        "            nn.Linear(256, 10)                 # 3rd fully connected layer: 512 input features, 10 output features (classes 0-9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)                    # Flatten the input tensor\n",
        "        logits = self.linear_relu_stack(x)      # Pass the flattened input through the sequential layers\n",
        "        return logits                          # Return the raw output (logits)\n",
        "\n",
        "# Create an instance of the NeuralNetwork model and move it to the selected device\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "# Print the model structure\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "If39-l85zxx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf3536b-4c68-4b12-9110-0013780257e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "AiE3L81-PHjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)  # Total number of samples in the dataset\n",
        "    model.train()                   # Set the model to training mode\n",
        "    for batch, (X, y) in enumerate(dataloader):  # Iterate over batches in the dataloader\n",
        "        X, y = X.to(device), y.to(device)       # Move data batches to the selected device (CPU or GPU)\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        pred = model(X)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation: calculate gradients of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters using the gradients and the chosen optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the gradients to prevent them from accumulating in subsequent iterations\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print training progress\n",
        "        if batch % 100 == 0:\n",
        "            # Retrieve the loss value and calculate the current batch size\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            # Print current batch loss and the number of processed samples out of the total\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ],
      "metadata": {
        "id": "Y-PjSlHY6dTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    # Get the total number of samples in the dataloader dataset\n",
        "    size = len(dataloader.dataset)\n",
        "    # Get the number of batches in the dataloader\n",
        "    num_batches = len(dataloader)\n",
        "    # Set the model to evaluation mode (disables dropout and batch normalization)\n",
        "    model.eval()\n",
        "    # Initialize variables to track the total loss and number of correct predictions\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Disable gradient calculation for efficiency during inference\n",
        "    with torch.no_grad():\n",
        "        # Iterate over batches in the dataloader\n",
        "        for X, y in dataloader:\n",
        "            # Move data and labels to the specified device (e.g., GPU or CPU)\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Perform the forward pass to get model predictions\n",
        "            pred = model(X)\n",
        "            # Calculate the loss for the current batch and add it to the total loss\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # Count the number of correct predictions by comparing the predicted labels\n",
        "            # (obtained via argmax) to the true labels and summing the results\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Calculate the average loss over all batches\n",
        "    test_loss /= num_batches\n",
        "    # Calculate the accuracy as the ratio of correct predictions to the total number of samples\n",
        "    correct /= size\n",
        "\n",
        "    # Print the test error, including accuracy (in percentage) and average loss\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ],
      "metadata": {
        "id": "2LBfi8hQN86g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs to train the model\n",
        "epochs = 100\n",
        "\n",
        "# Loop over each epoch\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    # Train the model using the training dataloader\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    # Test the model using the testing dataloader\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "\n",
        "# Print a message when training and testing are done\n",
        "print(\"Done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqB9RLuIOQpa",
        "outputId": "dd4bd916-58a5-4cca-9f06-1dd988ca7788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301228  [   64/60000]\n",
            "loss: 2.299524  [ 6464/60000]\n",
            "loss: 2.296021  [12864/60000]\n",
            "loss: 2.297302  [19264/60000]\n",
            "loss: 2.291381  [25664/60000]\n",
            "loss: 2.286667  [32064/60000]\n",
            "loss: 2.285662  [38464/60000]\n",
            "loss: 2.280249  [44864/60000]\n",
            "loss: 2.283090  [51264/60000]\n",
            "loss: 2.273833  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 24.5%, Avg loss: 2.278732 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.279610  [   64/60000]\n",
            "loss: 2.277636  [ 6464/60000]\n",
            "loss: 2.270198  [12864/60000]\n",
            "loss: 2.275278  [19264/60000]\n",
            "loss: 2.265346  [25664/60000]\n",
            "loss: 2.258263  [32064/60000]\n",
            "loss: 2.262432  [38464/60000]\n",
            "loss: 2.251502  [44864/60000]\n",
            "loss: 2.254095  [51264/60000]\n",
            "loss: 2.241081  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.1%, Avg loss: 2.244518 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.248223  [   64/60000]\n",
            "loss: 2.245344  [ 6464/60000]\n",
            "loss: 2.229025  [12864/60000]\n",
            "loss: 2.238471  [19264/60000]\n",
            "loss: 2.222417  [25664/60000]\n",
            "loss: 2.206857  [32064/60000]\n",
            "loss: 2.218655  [38464/60000]\n",
            "loss: 2.196387  [44864/60000]\n",
            "loss: 2.197385  [51264/60000]\n",
            "loss: 2.176335  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.9%, Avg loss: 2.178145 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.187595  [   64/60000]\n",
            "loss: 2.181604  [ 6464/60000]\n",
            "loss: 2.147532  [12864/60000]\n",
            "loss: 2.162628  [19264/60000]\n",
            "loss: 2.135280  [25664/60000]\n",
            "loss: 2.100632  [32064/60000]\n",
            "loss: 2.122574  [38464/60000]\n",
            "loss: 2.076788  [44864/60000]\n",
            "loss: 2.073707  [51264/60000]\n",
            "loss: 2.033253  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.8%, Avg loss: 2.033436 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.056521  [   64/60000]\n",
            "loss: 2.037225  [ 6464/60000]\n",
            "loss: 1.966711  [12864/60000]\n",
            "loss: 1.987354  [19264/60000]\n",
            "loss: 1.930912  [25664/60000]\n",
            "loss: 1.877416  [32064/60000]\n",
            "loss: 1.895350  [38464/60000]\n",
            "loss: 1.813672  [44864/60000]\n",
            "loss: 1.809914  [51264/60000]\n",
            "loss: 1.730074  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.1%, Avg loss: 1.737762 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.793175  [   64/60000]\n",
            "loss: 1.746264  [ 6464/60000]\n",
            "loss: 1.623087  [12864/60000]\n",
            "loss: 1.660034  [19264/60000]\n",
            "loss: 1.579881  [25664/60000]\n",
            "loss: 1.540708  [32064/60000]\n",
            "loss: 1.550127  [38464/60000]\n",
            "loss: 1.464006  [44864/60000]\n",
            "loss: 1.487170  [51264/60000]\n",
            "loss: 1.385408  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.4%, Avg loss: 1.411934 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.495981  [   64/60000]\n",
            "loss: 1.450488  [ 6464/60000]\n",
            "loss: 1.305379  [12864/60000]\n",
            "loss: 1.375804  [19264/60000]\n",
            "loss: 1.287874  [25664/60000]\n",
            "loss: 1.290538  [32064/60000]\n",
            "loss: 1.302005  [38464/60000]\n",
            "loss: 1.232478  [44864/60000]\n",
            "loss: 1.261451  [51264/60000]\n",
            "loss: 1.179367  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.5%, Avg loss: 1.200384 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.288172  [   64/60000]\n",
            "loss: 1.258449  [ 6464/60000]\n",
            "loss: 1.096967  [12864/60000]\n",
            "loss: 1.200871  [19264/60000]\n",
            "loss: 1.094741  [25664/60000]\n",
            "loss: 1.128986  [32064/60000]\n",
            "loss: 1.154161  [38464/60000]\n",
            "loss: 1.087464  [44864/60000]\n",
            "loss: 1.115146  [51264/60000]\n",
            "loss: 1.055569  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 1.065949 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.147991  [   64/60000]\n",
            "loss: 1.137886  [ 6464/60000]\n",
            "loss: 0.956151  [12864/60000]\n",
            "loss: 1.085759  [19264/60000]\n",
            "loss: 0.970957  [25664/60000]\n",
            "loss: 1.013337  [32064/60000]\n",
            "loss: 1.058220  [38464/60000]\n",
            "loss: 0.992179  [44864/60000]\n",
            "loss: 1.011479  [51264/60000]\n",
            "loss: 0.973437  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.3%, Avg loss: 0.974909 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.044347  [   64/60000]\n",
            "loss: 1.055932  [ 6464/60000]\n",
            "loss: 0.856033  [12864/60000]\n",
            "loss: 1.004797  [19264/60000]\n",
            "loss: 0.888459  [25664/60000]\n",
            "loss: 0.924589  [32064/60000]\n",
            "loss: 0.990424  [38464/60000]\n",
            "loss: 0.928945  [44864/60000]\n",
            "loss: 0.934139  [51264/60000]\n",
            "loss: 0.914556  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.5%, Avg loss: 0.909461 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.963426  [   64/60000]\n",
            "loss: 0.994659  [ 6464/60000]\n",
            "loss: 0.781419  [12864/60000]\n",
            "loss: 0.943527  [19264/60000]\n",
            "loss: 0.830617  [25664/60000]\n",
            "loss: 0.854999  [32064/60000]\n",
            "loss: 0.939300  [38464/60000]\n",
            "loss: 0.885575  [44864/60000]\n",
            "loss: 0.876262  [51264/60000]\n",
            "loss: 0.869803  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.860345 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.898844  [   64/60000]\n",
            "loss: 0.945899  [ 6464/60000]\n",
            "loss: 0.723992  [12864/60000]\n",
            "loss: 0.895570  [19264/60000]\n",
            "loss: 0.787820  [25664/60000]\n",
            "loss: 0.800522  [32064/60000]\n",
            "loss: 0.898970  [38464/60000]\n",
            "loss: 0.854859  [44864/60000]\n",
            "loss: 0.832758  [51264/60000]\n",
            "loss: 0.834041  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.822352 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.846273  [   64/60000]\n",
            "loss: 0.905215  [ 6464/60000]\n",
            "loss: 0.678634  [12864/60000]\n",
            "loss: 0.857975  [19264/60000]\n",
            "loss: 0.754760  [25664/60000]\n",
            "loss: 0.757604  [32064/60000]\n",
            "loss: 0.865896  [38464/60000]\n",
            "loss: 0.831633  [44864/60000]\n",
            "loss: 0.799892  [51264/60000]\n",
            "loss: 0.804854  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.792076 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.802756  [   64/60000]\n",
            "loss: 0.870234  [ 6464/60000]\n",
            "loss: 0.641787  [12864/60000]\n",
            "loss: 0.828234  [19264/60000]\n",
            "loss: 0.728148  [25664/60000]\n",
            "loss: 0.723899  [32064/60000]\n",
            "loss: 0.837861  [38464/60000]\n",
            "loss: 0.812552  [44864/60000]\n",
            "loss: 0.774266  [51264/60000]\n",
            "loss: 0.780267  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.767200 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.765880  [   64/60000]\n",
            "loss: 0.839776  [ 6464/60000]\n",
            "loss: 0.611167  [12864/60000]\n",
            "loss: 0.804649  [19264/60000]\n",
            "loss: 0.705851  [25664/60000]\n",
            "loss: 0.696920  [32064/60000]\n",
            "loss: 0.813143  [38464/60000]\n",
            "loss: 0.795920  [44864/60000]\n",
            "loss: 0.753710  [51264/60000]\n",
            "loss: 0.759045  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.746093 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.734069  [   64/60000]\n",
            "loss: 0.813100  [ 6464/60000]\n",
            "loss: 0.585860  [12864/60000]\n",
            "loss: 0.785314  [19264/60000]\n",
            "loss: 0.687607  [25664/60000]\n",
            "loss: 0.675146  [32064/60000]\n",
            "loss: 0.790693  [38464/60000]\n",
            "loss: 0.780594  [44864/60000]\n",
            "loss: 0.736839  [51264/60000]\n",
            "loss: 0.740720  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.727679 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.706349  [   64/60000]\n",
            "loss: 0.789882  [ 6464/60000]\n",
            "loss: 0.564312  [12864/60000]\n",
            "loss: 0.768941  [19264/60000]\n",
            "loss: 0.672576  [25664/60000]\n",
            "loss: 0.657285  [32064/60000]\n",
            "loss: 0.769956  [38464/60000]\n",
            "loss: 0.765990  [44864/60000]\n",
            "loss: 0.722544  [51264/60000]\n",
            "loss: 0.724469  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.711205 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.681855  [   64/60000]\n",
            "loss: 0.769550  [ 6464/60000]\n",
            "loss: 0.545730  [12864/60000]\n",
            "loss: 0.754529  [19264/60000]\n",
            "loss: 0.660247  [25664/60000]\n",
            "loss: 0.642504  [32064/60000]\n",
            "loss: 0.750553  [38464/60000]\n",
            "loss: 0.752183  [44864/60000]\n",
            "loss: 0.709872  [51264/60000]\n",
            "loss: 0.709895  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.696257 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.659996  [   64/60000]\n",
            "loss: 0.751553  [ 6464/60000]\n",
            "loss: 0.529435  [12864/60000]\n",
            "loss: 0.741506  [19264/60000]\n",
            "loss: 0.650108  [25664/60000]\n",
            "loss: 0.630133  [32064/60000]\n",
            "loss: 0.732572  [38464/60000]\n",
            "loss: 0.739275  [44864/60000]\n",
            "loss: 0.698701  [51264/60000]\n",
            "loss: 0.696465  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.682579 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.640556  [   64/60000]\n",
            "loss: 0.735532  [ 6464/60000]\n",
            "loss: 0.514979  [12864/60000]\n",
            "loss: 0.729507  [19264/60000]\n",
            "loss: 0.642054  [25664/60000]\n",
            "loss: 0.619632  [32064/60000]\n",
            "loss: 0.715809  [38464/60000]\n",
            "loss: 0.727340  [44864/60000]\n",
            "loss: 0.688671  [51264/60000]\n",
            "loss: 0.684155  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.1%, Avg loss: 0.670054 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.623212  [   64/60000]\n",
            "loss: 0.721061  [ 6464/60000]\n",
            "loss: 0.502065  [12864/60000]\n",
            "loss: 0.718338  [19264/60000]\n",
            "loss: 0.635567  [25664/60000]\n",
            "loss: 0.610741  [32064/60000]\n",
            "loss: 0.700268  [38464/60000]\n",
            "loss: 0.716568  [44864/60000]\n",
            "loss: 0.679716  [51264/60000]\n",
            "loss: 0.672672  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.658589 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.607597  [   64/60000]\n",
            "loss: 0.708137  [ 6464/60000]\n",
            "loss: 0.490446  [12864/60000]\n",
            "loss: 0.707945  [19264/60000]\n",
            "loss: 0.630531  [25664/60000]\n",
            "loss: 0.603078  [32064/60000]\n",
            "loss: 0.685826  [38464/60000]\n",
            "loss: 0.707189  [44864/60000]\n",
            "loss: 0.671849  [51264/60000]\n",
            "loss: 0.661876  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.648071 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.593524  [   64/60000]\n",
            "loss: 0.696442  [ 6464/60000]\n",
            "loss: 0.479972  [12864/60000]\n",
            "loss: 0.698172  [19264/60000]\n",
            "loss: 0.626417  [25664/60000]\n",
            "loss: 0.596511  [32064/60000]\n",
            "loss: 0.672322  [38464/60000]\n",
            "loss: 0.699004  [44864/60000]\n",
            "loss: 0.664901  [51264/60000]\n",
            "loss: 0.651603  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.638399 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.580644  [   64/60000]\n",
            "loss: 0.685678  [ 6464/60000]\n",
            "loss: 0.470435  [12864/60000]\n",
            "loss: 0.688897  [19264/60000]\n",
            "loss: 0.622958  [25664/60000]\n",
            "loss: 0.590713  [32064/60000]\n",
            "loss: 0.659770  [38464/60000]\n",
            "loss: 0.692000  [44864/60000]\n",
            "loss: 0.658753  [51264/60000]\n",
            "loss: 0.641807  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.629484 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.568795  [   64/60000]\n",
            "loss: 0.675789  [ 6464/60000]\n",
            "loss: 0.461677  [12864/60000]\n",
            "loss: 0.680006  [19264/60000]\n",
            "loss: 0.619866  [25664/60000]\n",
            "loss: 0.585661  [32064/60000]\n",
            "loss: 0.648176  [38464/60000]\n",
            "loss: 0.686139  [44864/60000]\n",
            "loss: 0.653170  [51264/60000]\n",
            "loss: 0.632577  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.9%, Avg loss: 0.621239 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.557962  [   64/60000]\n",
            "loss: 0.666729  [ 6464/60000]\n",
            "loss: 0.453523  [12864/60000]\n",
            "loss: 0.671587  [19264/60000]\n",
            "loss: 0.616948  [25664/60000]\n",
            "loss: 0.581033  [32064/60000]\n",
            "loss: 0.637422  [38464/60000]\n",
            "loss: 0.681381  [44864/60000]\n",
            "loss: 0.648196  [51264/60000]\n",
            "loss: 0.623697  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.613567 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.547889  [   64/60000]\n",
            "loss: 0.658368  [ 6464/60000]\n",
            "loss: 0.445968  [12864/60000]\n",
            "loss: 0.663480  [19264/60000]\n",
            "loss: 0.614131  [25664/60000]\n",
            "loss: 0.576615  [32064/60000]\n",
            "loss: 0.627275  [38464/60000]\n",
            "loss: 0.677409  [44864/60000]\n",
            "loss: 0.643522  [51264/60000]\n",
            "loss: 0.615116  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.606415 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.538265  [   64/60000]\n",
            "loss: 0.650617  [ 6464/60000]\n",
            "loss: 0.438920  [12864/60000]\n",
            "loss: 0.655779  [19264/60000]\n",
            "loss: 0.611225  [25664/60000]\n",
            "loss: 0.572385  [32064/60000]\n",
            "loss: 0.617749  [38464/60000]\n",
            "loss: 0.674222  [44864/60000]\n",
            "loss: 0.639304  [51264/60000]\n",
            "loss: 0.606832  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.6%, Avg loss: 0.599698 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.528921  [   64/60000]\n",
            "loss: 0.643215  [ 6464/60000]\n",
            "loss: 0.432233  [12864/60000]\n",
            "loss: 0.648373  [19264/60000]\n",
            "loss: 0.608096  [25664/60000]\n",
            "loss: 0.568355  [32064/60000]\n",
            "loss: 0.608792  [38464/60000]\n",
            "loss: 0.671587  [44864/60000]\n",
            "loss: 0.635290  [51264/60000]\n",
            "loss: 0.598793  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.593347 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.519779  [   64/60000]\n",
            "loss: 0.636121  [ 6464/60000]\n",
            "loss: 0.425676  [12864/60000]\n",
            "loss: 0.641159  [19264/60000]\n",
            "loss: 0.604441  [25664/60000]\n",
            "loss: 0.564306  [32064/60000]\n",
            "loss: 0.600174  [38464/60000]\n",
            "loss: 0.669401  [44864/60000]\n",
            "loss: 0.631485  [51264/60000]\n",
            "loss: 0.591021  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.3%, Avg loss: 0.587321 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.510776  [   64/60000]\n",
            "loss: 0.629441  [ 6464/60000]\n",
            "loss: 0.419437  [12864/60000]\n",
            "loss: 0.634099  [19264/60000]\n",
            "loss: 0.600660  [25664/60000]\n",
            "loss: 0.560228  [32064/60000]\n",
            "loss: 0.592095  [38464/60000]\n",
            "loss: 0.667765  [44864/60000]\n",
            "loss: 0.627895  [51264/60000]\n",
            "loss: 0.583232  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 0.581557 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.501882  [   64/60000]\n",
            "loss: 0.622931  [ 6464/60000]\n",
            "loss: 0.413565  [12864/60000]\n",
            "loss: 0.627042  [19264/60000]\n",
            "loss: 0.596528  [25664/60000]\n",
            "loss: 0.556068  [32064/60000]\n",
            "loss: 0.584564  [38464/60000]\n",
            "loss: 0.666627  [44864/60000]\n",
            "loss: 0.624599  [51264/60000]\n",
            "loss: 0.575505  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.576035 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.493125  [   64/60000]\n",
            "loss: 0.616685  [ 6464/60000]\n",
            "loss: 0.408003  [12864/60000]\n",
            "loss: 0.620088  [19264/60000]\n",
            "loss: 0.592256  [25664/60000]\n",
            "loss: 0.552024  [32064/60000]\n",
            "loss: 0.577552  [38464/60000]\n",
            "loss: 0.665847  [44864/60000]\n",
            "loss: 0.621374  [51264/60000]\n",
            "loss: 0.568045  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.570744 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.484479  [   64/60000]\n",
            "loss: 0.610817  [ 6464/60000]\n",
            "loss: 0.402804  [12864/60000]\n",
            "loss: 0.613132  [19264/60000]\n",
            "loss: 0.587779  [25664/60000]\n",
            "loss: 0.547871  [32064/60000]\n",
            "loss: 0.571061  [38464/60000]\n",
            "loss: 0.665259  [44864/60000]\n",
            "loss: 0.618357  [51264/60000]\n",
            "loss: 0.560862  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.565661 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.475973  [   64/60000]\n",
            "loss: 0.605089  [ 6464/60000]\n",
            "loss: 0.397911  [12864/60000]\n",
            "loss: 0.606313  [19264/60000]\n",
            "loss: 0.582892  [25664/60000]\n",
            "loss: 0.543762  [32064/60000]\n",
            "loss: 0.564929  [38464/60000]\n",
            "loss: 0.664865  [44864/60000]\n",
            "loss: 0.615533  [51264/60000]\n",
            "loss: 0.553970  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.560769 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.467627  [   64/60000]\n",
            "loss: 0.599426  [ 6464/60000]\n",
            "loss: 0.393264  [12864/60000]\n",
            "loss: 0.599767  [19264/60000]\n",
            "loss: 0.577744  [25664/60000]\n",
            "loss: 0.539635  [32064/60000]\n",
            "loss: 0.559128  [38464/60000]\n",
            "loss: 0.664711  [44864/60000]\n",
            "loss: 0.612708  [51264/60000]\n",
            "loss: 0.547202  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.556075 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.459442  [   64/60000]\n",
            "loss: 0.593927  [ 6464/60000]\n",
            "loss: 0.388787  [12864/60000]\n",
            "loss: 0.593312  [19264/60000]\n",
            "loss: 0.572359  [25664/60000]\n",
            "loss: 0.535313  [32064/60000]\n",
            "loss: 0.553625  [38464/60000]\n",
            "loss: 0.664833  [44864/60000]\n",
            "loss: 0.609972  [51264/60000]\n",
            "loss: 0.540585  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.551554 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.451492  [   64/60000]\n",
            "loss: 0.588550  [ 6464/60000]\n",
            "loss: 0.384446  [12864/60000]\n",
            "loss: 0.586868  [19264/60000]\n",
            "loss: 0.566756  [25664/60000]\n",
            "loss: 0.531058  [32064/60000]\n",
            "loss: 0.548468  [38464/60000]\n",
            "loss: 0.664921  [44864/60000]\n",
            "loss: 0.607308  [51264/60000]\n",
            "loss: 0.534023  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.547210 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.443824  [   64/60000]\n",
            "loss: 0.583312  [ 6464/60000]\n",
            "loss: 0.380229  [12864/60000]\n",
            "loss: 0.580632  [19264/60000]\n",
            "loss: 0.560951  [25664/60000]\n",
            "loss: 0.526899  [32064/60000]\n",
            "loss: 0.543572  [38464/60000]\n",
            "loss: 0.665043  [44864/60000]\n",
            "loss: 0.604782  [51264/60000]\n",
            "loss: 0.527818  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.543042 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.436317  [   64/60000]\n",
            "loss: 0.578211  [ 6464/60000]\n",
            "loss: 0.376176  [12864/60000]\n",
            "loss: 0.574606  [19264/60000]\n",
            "loss: 0.555116  [25664/60000]\n",
            "loss: 0.522803  [32064/60000]\n",
            "loss: 0.538909  [38464/60000]\n",
            "loss: 0.665345  [44864/60000]\n",
            "loss: 0.602397  [51264/60000]\n",
            "loss: 0.521766  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.539036 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.429071  [   64/60000]\n",
            "loss: 0.573342  [ 6464/60000]\n",
            "loss: 0.372191  [12864/60000]\n",
            "loss: 0.568712  [19264/60000]\n",
            "loss: 0.548947  [25664/60000]\n",
            "loss: 0.518769  [32064/60000]\n",
            "loss: 0.534524  [38464/60000]\n",
            "loss: 0.665657  [44864/60000]\n",
            "loss: 0.600205  [51264/60000]\n",
            "loss: 0.515961  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.535194 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.422105  [   64/60000]\n",
            "loss: 0.568599  [ 6464/60000]\n",
            "loss: 0.368326  [12864/60000]\n",
            "loss: 0.563155  [19264/60000]\n",
            "loss: 0.542746  [25664/60000]\n",
            "loss: 0.514884  [32064/60000]\n",
            "loss: 0.530361  [38464/60000]\n",
            "loss: 0.666092  [44864/60000]\n",
            "loss: 0.598226  [51264/60000]\n",
            "loss: 0.510342  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.531510 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.415338  [   64/60000]\n",
            "loss: 0.563948  [ 6464/60000]\n",
            "loss: 0.364601  [12864/60000]\n",
            "loss: 0.557760  [19264/60000]\n",
            "loss: 0.536441  [25664/60000]\n",
            "loss: 0.510987  [32064/60000]\n",
            "loss: 0.526397  [38464/60000]\n",
            "loss: 0.666512  [44864/60000]\n",
            "loss: 0.596165  [51264/60000]\n",
            "loss: 0.505095  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.527979 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.408841  [   64/60000]\n",
            "loss: 0.559567  [ 6464/60000]\n",
            "loss: 0.360978  [12864/60000]\n",
            "loss: 0.552536  [19264/60000]\n",
            "loss: 0.529986  [25664/60000]\n",
            "loss: 0.507051  [32064/60000]\n",
            "loss: 0.522653  [38464/60000]\n",
            "loss: 0.666950  [44864/60000]\n",
            "loss: 0.594139  [51264/60000]\n",
            "loss: 0.499997  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.524578 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.402659  [   64/60000]\n",
            "loss: 0.555370  [ 6464/60000]\n",
            "loss: 0.357524  [12864/60000]\n",
            "loss: 0.547471  [19264/60000]\n",
            "loss: 0.523614  [25664/60000]\n",
            "loss: 0.503352  [32064/60000]\n",
            "loss: 0.519062  [38464/60000]\n",
            "loss: 0.667192  [44864/60000]\n",
            "loss: 0.591830  [51264/60000]\n",
            "loss: 0.495254  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.521328 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.396579  [   64/60000]\n",
            "loss: 0.551356  [ 6464/60000]\n",
            "loss: 0.354102  [12864/60000]\n",
            "loss: 0.542514  [19264/60000]\n",
            "loss: 0.517192  [25664/60000]\n",
            "loss: 0.499751  [32064/60000]\n",
            "loss: 0.515572  [38464/60000]\n",
            "loss: 0.667328  [44864/60000]\n",
            "loss: 0.589594  [51264/60000]\n",
            "loss: 0.490657  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.518237 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.390605  [   64/60000]\n",
            "loss: 0.547536  [ 6464/60000]\n",
            "loss: 0.350718  [12864/60000]\n",
            "loss: 0.537725  [19264/60000]\n",
            "loss: 0.510834  [25664/60000]\n",
            "loss: 0.496233  [32064/60000]\n",
            "loss: 0.512203  [38464/60000]\n",
            "loss: 0.667484  [44864/60000]\n",
            "loss: 0.587346  [51264/60000]\n",
            "loss: 0.486251  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.515279 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.384848  [   64/60000]\n",
            "loss: 0.543885  [ 6464/60000]\n",
            "loss: 0.347474  [12864/60000]\n",
            "loss: 0.533150  [19264/60000]\n",
            "loss: 0.504531  [25664/60000]\n",
            "loss: 0.492866  [32064/60000]\n",
            "loss: 0.508970  [38464/60000]\n",
            "loss: 0.667524  [44864/60000]\n",
            "loss: 0.585104  [51264/60000]\n",
            "loss: 0.482003  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.512456 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.379289  [   64/60000]\n",
            "loss: 0.540340  [ 6464/60000]\n",
            "loss: 0.344337  [12864/60000]\n",
            "loss: 0.528821  [19264/60000]\n",
            "loss: 0.498406  [25664/60000]\n",
            "loss: 0.489650  [32064/60000]\n",
            "loss: 0.505904  [38464/60000]\n",
            "loss: 0.667418  [44864/60000]\n",
            "loss: 0.582877  [51264/60000]\n",
            "loss: 0.478020  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.509731 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.374012  [   64/60000]\n",
            "loss: 0.536858  [ 6464/60000]\n",
            "loss: 0.341321  [12864/60000]\n",
            "loss: 0.524674  [19264/60000]\n",
            "loss: 0.492365  [25664/60000]\n",
            "loss: 0.486592  [32064/60000]\n",
            "loss: 0.502891  [38464/60000]\n",
            "loss: 0.667091  [44864/60000]\n",
            "loss: 0.580614  [51264/60000]\n",
            "loss: 0.474185  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.507114 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.368883  [   64/60000]\n",
            "loss: 0.533619  [ 6464/60000]\n",
            "loss: 0.338405  [12864/60000]\n",
            "loss: 0.520748  [19264/60000]\n",
            "loss: 0.486420  [25664/60000]\n",
            "loss: 0.483576  [32064/60000]\n",
            "loss: 0.499943  [38464/60000]\n",
            "loss: 0.666570  [44864/60000]\n",
            "loss: 0.578355  [51264/60000]\n",
            "loss: 0.470551  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.504597 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.363928  [   64/60000]\n",
            "loss: 0.530461  [ 6464/60000]\n",
            "loss: 0.335523  [12864/60000]\n",
            "loss: 0.516953  [19264/60000]\n",
            "loss: 0.480774  [25664/60000]\n",
            "loss: 0.480777  [32064/60000]\n",
            "loss: 0.497047  [38464/60000]\n",
            "loss: 0.665884  [44864/60000]\n",
            "loss: 0.576103  [51264/60000]\n",
            "loss: 0.467151  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.502182 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.359208  [   64/60000]\n",
            "loss: 0.527398  [ 6464/60000]\n",
            "loss: 0.332776  [12864/60000]\n",
            "loss: 0.513388  [19264/60000]\n",
            "loss: 0.475268  [25664/60000]\n",
            "loss: 0.478029  [32064/60000]\n",
            "loss: 0.494251  [38464/60000]\n",
            "loss: 0.664947  [44864/60000]\n",
            "loss: 0.573754  [51264/60000]\n",
            "loss: 0.463898  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.499851 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.354616  [   64/60000]\n",
            "loss: 0.524415  [ 6464/60000]\n",
            "loss: 0.330146  [12864/60000]\n",
            "loss: 0.509943  [19264/60000]\n",
            "loss: 0.469848  [25664/60000]\n",
            "loss: 0.475489  [32064/60000]\n",
            "loss: 0.491563  [38464/60000]\n",
            "loss: 0.663820  [44864/60000]\n",
            "loss: 0.571432  [51264/60000]\n",
            "loss: 0.460876  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.497595 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.350198  [   64/60000]\n",
            "loss: 0.521496  [ 6464/60000]\n",
            "loss: 0.327623  [12864/60000]\n",
            "loss: 0.506673  [19264/60000]\n",
            "loss: 0.464595  [25664/60000]\n",
            "loss: 0.472947  [32064/60000]\n",
            "loss: 0.488905  [38464/60000]\n",
            "loss: 0.662582  [44864/60000]\n",
            "loss: 0.569192  [51264/60000]\n",
            "loss: 0.457984  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.495417 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.345969  [   64/60000]\n",
            "loss: 0.518670  [ 6464/60000]\n",
            "loss: 0.325069  [12864/60000]\n",
            "loss: 0.503582  [19264/60000]\n",
            "loss: 0.459385  [25664/60000]\n",
            "loss: 0.470602  [32064/60000]\n",
            "loss: 0.486329  [38464/60000]\n",
            "loss: 0.661148  [44864/60000]\n",
            "loss: 0.566903  [51264/60000]\n",
            "loss: 0.455189  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.493312 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.341901  [   64/60000]\n",
            "loss: 0.516009  [ 6464/60000]\n",
            "loss: 0.322661  [12864/60000]\n",
            "loss: 0.500690  [19264/60000]\n",
            "loss: 0.454333  [25664/60000]\n",
            "loss: 0.468246  [32064/60000]\n",
            "loss: 0.483770  [38464/60000]\n",
            "loss: 0.659592  [44864/60000]\n",
            "loss: 0.564667  [51264/60000]\n",
            "loss: 0.452680  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.491279 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.338005  [   64/60000]\n",
            "loss: 0.513397  [ 6464/60000]\n",
            "loss: 0.320293  [12864/60000]\n",
            "loss: 0.497901  [19264/60000]\n",
            "loss: 0.449512  [25664/60000]\n",
            "loss: 0.466151  [32064/60000]\n",
            "loss: 0.481211  [38464/60000]\n",
            "loss: 0.657948  [44864/60000]\n",
            "loss: 0.562416  [51264/60000]\n",
            "loss: 0.450251  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.489310 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.334270  [   64/60000]\n",
            "loss: 0.510891  [ 6464/60000]\n",
            "loss: 0.317991  [12864/60000]\n",
            "loss: 0.495215  [19264/60000]\n",
            "loss: 0.444781  [25664/60000]\n",
            "loss: 0.464057  [32064/60000]\n",
            "loss: 0.478688  [38464/60000]\n",
            "loss: 0.656245  [44864/60000]\n",
            "loss: 0.560136  [51264/60000]\n",
            "loss: 0.447918  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.487376 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.330633  [   64/60000]\n",
            "loss: 0.508384  [ 6464/60000]\n",
            "loss: 0.315849  [12864/60000]\n",
            "loss: 0.492725  [19264/60000]\n",
            "loss: 0.440296  [25664/60000]\n",
            "loss: 0.462073  [32064/60000]\n",
            "loss: 0.476138  [38464/60000]\n",
            "loss: 0.654489  [44864/60000]\n",
            "loss: 0.557771  [51264/60000]\n",
            "loss: 0.445791  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.485488 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.327192  [   64/60000]\n",
            "loss: 0.505990  [ 6464/60000]\n",
            "loss: 0.313652  [12864/60000]\n",
            "loss: 0.490351  [19264/60000]\n",
            "loss: 0.435820  [25664/60000]\n",
            "loss: 0.460217  [32064/60000]\n",
            "loss: 0.473657  [38464/60000]\n",
            "loss: 0.652756  [44864/60000]\n",
            "loss: 0.555484  [51264/60000]\n",
            "loss: 0.443662  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.483639 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.323741  [   64/60000]\n",
            "loss: 0.503589  [ 6464/60000]\n",
            "loss: 0.311630  [12864/60000]\n",
            "loss: 0.488127  [19264/60000]\n",
            "loss: 0.431759  [25664/60000]\n",
            "loss: 0.458446  [32064/60000]\n",
            "loss: 0.471213  [38464/60000]\n",
            "loss: 0.650869  [44864/60000]\n",
            "loss: 0.553005  [51264/60000]\n",
            "loss: 0.441413  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.481810 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.320470  [   64/60000]\n",
            "loss: 0.501090  [ 6464/60000]\n",
            "loss: 0.309582  [12864/60000]\n",
            "loss: 0.486091  [19264/60000]\n",
            "loss: 0.427706  [25664/60000]\n",
            "loss: 0.456510  [32064/60000]\n",
            "loss: 0.468654  [38464/60000]\n",
            "loss: 0.648521  [44864/60000]\n",
            "loss: 0.550437  [51264/60000]\n",
            "loss: 0.439283  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.480005 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.317144  [   64/60000]\n",
            "loss: 0.498634  [ 6464/60000]\n",
            "loss: 0.307349  [12864/60000]\n",
            "loss: 0.484062  [19264/60000]\n",
            "loss: 0.423595  [25664/60000]\n",
            "loss: 0.454753  [32064/60000]\n",
            "loss: 0.465997  [38464/60000]\n",
            "loss: 0.646360  [44864/60000]\n",
            "loss: 0.547846  [51264/60000]\n",
            "loss: 0.437257  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.478254 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.314105  [   64/60000]\n",
            "loss: 0.496246  [ 6464/60000]\n",
            "loss: 0.305373  [12864/60000]\n",
            "loss: 0.482129  [19264/60000]\n",
            "loss: 0.419684  [25664/60000]\n",
            "loss: 0.453159  [32064/60000]\n",
            "loss: 0.463641  [38464/60000]\n",
            "loss: 0.644392  [44864/60000]\n",
            "loss: 0.545340  [51264/60000]\n",
            "loss: 0.435494  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.476585 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.311152  [   64/60000]\n",
            "loss: 0.493863  [ 6464/60000]\n",
            "loss: 0.303433  [12864/60000]\n",
            "loss: 0.480286  [19264/60000]\n",
            "loss: 0.415835  [25664/60000]\n",
            "loss: 0.451677  [32064/60000]\n",
            "loss: 0.461305  [38464/60000]\n",
            "loss: 0.642335  [44864/60000]\n",
            "loss: 0.543076  [51264/60000]\n",
            "loss: 0.433725  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.474961 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.308412  [   64/60000]\n",
            "loss: 0.491474  [ 6464/60000]\n",
            "loss: 0.301533  [12864/60000]\n",
            "loss: 0.478519  [19264/60000]\n",
            "loss: 0.412042  [25664/60000]\n",
            "loss: 0.450261  [32064/60000]\n",
            "loss: 0.459023  [38464/60000]\n",
            "loss: 0.640294  [44864/60000]\n",
            "loss: 0.540811  [51264/60000]\n",
            "loss: 0.432104  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.473369 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.305708  [   64/60000]\n",
            "loss: 0.489158  [ 6464/60000]\n",
            "loss: 0.299667  [12864/60000]\n",
            "loss: 0.476784  [19264/60000]\n",
            "loss: 0.408426  [25664/60000]\n",
            "loss: 0.448907  [32064/60000]\n",
            "loss: 0.456813  [38464/60000]\n",
            "loss: 0.638191  [44864/60000]\n",
            "loss: 0.538587  [51264/60000]\n",
            "loss: 0.430601  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.471812 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.303113  [   64/60000]\n",
            "loss: 0.486774  [ 6464/60000]\n",
            "loss: 0.297960  [12864/60000]\n",
            "loss: 0.475143  [19264/60000]\n",
            "loss: 0.404852  [25664/60000]\n",
            "loss: 0.447552  [32064/60000]\n",
            "loss: 0.454671  [38464/60000]\n",
            "loss: 0.636116  [44864/60000]\n",
            "loss: 0.536381  [51264/60000]\n",
            "loss: 0.429193  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.470264 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.300639  [   64/60000]\n",
            "loss: 0.484414  [ 6464/60000]\n",
            "loss: 0.296232  [12864/60000]\n",
            "loss: 0.473470  [19264/60000]\n",
            "loss: 0.401310  [25664/60000]\n",
            "loss: 0.446238  [32064/60000]\n",
            "loss: 0.452647  [38464/60000]\n",
            "loss: 0.634037  [44864/60000]\n",
            "loss: 0.534160  [51264/60000]\n",
            "loss: 0.427850  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.468760 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.298217  [   64/60000]\n",
            "loss: 0.482123  [ 6464/60000]\n",
            "loss: 0.294483  [12864/60000]\n",
            "loss: 0.471800  [19264/60000]\n",
            "loss: 0.397738  [25664/60000]\n",
            "loss: 0.444972  [32064/60000]\n",
            "loss: 0.450593  [38464/60000]\n",
            "loss: 0.631938  [44864/60000]\n",
            "loss: 0.532145  [51264/60000]\n",
            "loss: 0.426617  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.467273 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.295901  [   64/60000]\n",
            "loss: 0.479701  [ 6464/60000]\n",
            "loss: 0.292844  [12864/60000]\n",
            "loss: 0.470182  [19264/60000]\n",
            "loss: 0.394480  [25664/60000]\n",
            "loss: 0.443727  [32064/60000]\n",
            "loss: 0.448757  [38464/60000]\n",
            "loss: 0.629853  [44864/60000]\n",
            "loss: 0.530079  [51264/60000]\n",
            "loss: 0.425298  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.465839 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.293658  [   64/60000]\n",
            "loss: 0.477441  [ 6464/60000]\n",
            "loss: 0.291338  [12864/60000]\n",
            "loss: 0.468796  [19264/60000]\n",
            "loss: 0.391175  [25664/60000]\n",
            "loss: 0.442528  [32064/60000]\n",
            "loss: 0.446982  [38464/60000]\n",
            "loss: 0.627832  [44864/60000]\n",
            "loss: 0.527843  [51264/60000]\n",
            "loss: 0.423893  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.464453 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.291430  [   64/60000]\n",
            "loss: 0.475253  [ 6464/60000]\n",
            "loss: 0.289770  [12864/60000]\n",
            "loss: 0.467370  [19264/60000]\n",
            "loss: 0.387947  [25664/60000]\n",
            "loss: 0.441218  [32064/60000]\n",
            "loss: 0.445266  [38464/60000]\n",
            "loss: 0.625807  [44864/60000]\n",
            "loss: 0.525704  [51264/60000]\n",
            "loss: 0.422609  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.463082 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.289335  [   64/60000]\n",
            "loss: 0.473141  [ 6464/60000]\n",
            "loss: 0.288196  [12864/60000]\n",
            "loss: 0.465915  [19264/60000]\n",
            "loss: 0.384803  [25664/60000]\n",
            "loss: 0.439890  [32064/60000]\n",
            "loss: 0.443592  [38464/60000]\n",
            "loss: 0.623688  [44864/60000]\n",
            "loss: 0.523543  [51264/60000]\n",
            "loss: 0.421286  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.461735 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.287362  [   64/60000]\n",
            "loss: 0.471012  [ 6464/60000]\n",
            "loss: 0.286765  [12864/60000]\n",
            "loss: 0.464579  [19264/60000]\n",
            "loss: 0.381796  [25664/60000]\n",
            "loss: 0.438633  [32064/60000]\n",
            "loss: 0.441933  [38464/60000]\n",
            "loss: 0.621684  [44864/60000]\n",
            "loss: 0.521498  [51264/60000]\n",
            "loss: 0.419982  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.460438 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.285446  [   64/60000]\n",
            "loss: 0.468885  [ 6464/60000]\n",
            "loss: 0.285393  [12864/60000]\n",
            "loss: 0.463232  [19264/60000]\n",
            "loss: 0.378832  [25664/60000]\n",
            "loss: 0.437384  [32064/60000]\n",
            "loss: 0.440086  [38464/60000]\n",
            "loss: 0.619655  [44864/60000]\n",
            "loss: 0.519486  [51264/60000]\n",
            "loss: 0.418783  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.459140 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.283559  [   64/60000]\n",
            "loss: 0.466740  [ 6464/60000]\n",
            "loss: 0.284054  [12864/60000]\n",
            "loss: 0.461817  [19264/60000]\n",
            "loss: 0.375916  [25664/60000]\n",
            "loss: 0.436222  [32064/60000]\n",
            "loss: 0.438278  [38464/60000]\n",
            "loss: 0.617615  [44864/60000]\n",
            "loss: 0.517130  [51264/60000]\n",
            "loss: 0.417551  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.457861 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.281779  [   64/60000]\n",
            "loss: 0.464579  [ 6464/60000]\n",
            "loss: 0.282738  [12864/60000]\n",
            "loss: 0.460374  [19264/60000]\n",
            "loss: 0.373144  [25664/60000]\n",
            "loss: 0.435141  [32064/60000]\n",
            "loss: 0.436673  [38464/60000]\n",
            "loss: 0.615669  [44864/60000]\n",
            "loss: 0.515001  [51264/60000]\n",
            "loss: 0.416454  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.456600 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.280074  [   64/60000]\n",
            "loss: 0.462410  [ 6464/60000]\n",
            "loss: 0.281398  [12864/60000]\n",
            "loss: 0.458995  [19264/60000]\n",
            "loss: 0.370478  [25664/60000]\n",
            "loss: 0.434088  [32064/60000]\n",
            "loss: 0.435079  [38464/60000]\n",
            "loss: 0.613667  [44864/60000]\n",
            "loss: 0.512940  [51264/60000]\n",
            "loss: 0.415425  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.455358 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.278402  [   64/60000]\n",
            "loss: 0.460215  [ 6464/60000]\n",
            "loss: 0.280080  [12864/60000]\n",
            "loss: 0.457628  [19264/60000]\n",
            "loss: 0.367876  [25664/60000]\n",
            "loss: 0.432968  [32064/60000]\n",
            "loss: 0.433498  [38464/60000]\n",
            "loss: 0.611814  [44864/60000]\n",
            "loss: 0.510947  [51264/60000]\n",
            "loss: 0.414482  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 0.454134 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.276803  [   64/60000]\n",
            "loss: 0.458073  [ 6464/60000]\n",
            "loss: 0.278780  [12864/60000]\n",
            "loss: 0.456256  [19264/60000]\n",
            "loss: 0.365267  [25664/60000]\n",
            "loss: 0.431858  [32064/60000]\n",
            "loss: 0.431830  [38464/60000]\n",
            "loss: 0.609854  [44864/60000]\n",
            "loss: 0.508932  [51264/60000]\n",
            "loss: 0.413586  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.452931 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.275178  [   64/60000]\n",
            "loss: 0.455982  [ 6464/60000]\n",
            "loss: 0.277567  [12864/60000]\n",
            "loss: 0.454893  [19264/60000]\n",
            "loss: 0.362676  [25664/60000]\n",
            "loss: 0.430744  [32064/60000]\n",
            "loss: 0.430230  [38464/60000]\n",
            "loss: 0.607978  [44864/60000]\n",
            "loss: 0.507033  [51264/60000]\n",
            "loss: 0.412594  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.451736 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.273592  [   64/60000]\n",
            "loss: 0.453910  [ 6464/60000]\n",
            "loss: 0.276436  [12864/60000]\n",
            "loss: 0.453576  [19264/60000]\n",
            "loss: 0.360265  [25664/60000]\n",
            "loss: 0.429582  [32064/60000]\n",
            "loss: 0.428722  [38464/60000]\n",
            "loss: 0.606124  [44864/60000]\n",
            "loss: 0.505007  [51264/60000]\n",
            "loss: 0.411502  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.450567 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.272054  [   64/60000]\n",
            "loss: 0.451917  [ 6464/60000]\n",
            "loss: 0.275332  [12864/60000]\n",
            "loss: 0.452276  [19264/60000]\n",
            "loss: 0.357886  [25664/60000]\n",
            "loss: 0.428488  [32064/60000]\n",
            "loss: 0.427197  [38464/60000]\n",
            "loss: 0.604368  [44864/60000]\n",
            "loss: 0.503270  [51264/60000]\n",
            "loss: 0.410359  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.449409 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.270539  [   64/60000]\n",
            "loss: 0.449902  [ 6464/60000]\n",
            "loss: 0.274250  [12864/60000]\n",
            "loss: 0.451064  [19264/60000]\n",
            "loss: 0.355552  [25664/60000]\n",
            "loss: 0.427323  [32064/60000]\n",
            "loss: 0.425700  [38464/60000]\n",
            "loss: 0.602529  [44864/60000]\n",
            "loss: 0.501300  [51264/60000]\n",
            "loss: 0.409293  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.448257 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.269092  [   64/60000]\n",
            "loss: 0.447939  [ 6464/60000]\n",
            "loss: 0.273204  [12864/60000]\n",
            "loss: 0.449724  [19264/60000]\n",
            "loss: 0.353208  [25664/60000]\n",
            "loss: 0.426080  [32064/60000]\n",
            "loss: 0.424306  [38464/60000]\n",
            "loss: 0.600779  [44864/60000]\n",
            "loss: 0.499384  [51264/60000]\n",
            "loss: 0.408423  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.447131 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.267760  [   64/60000]\n",
            "loss: 0.445982  [ 6464/60000]\n",
            "loss: 0.272142  [12864/60000]\n",
            "loss: 0.448419  [19264/60000]\n",
            "loss: 0.350939  [25664/60000]\n",
            "loss: 0.424935  [32064/60000]\n",
            "loss: 0.422876  [38464/60000]\n",
            "loss: 0.599088  [44864/60000]\n",
            "loss: 0.497501  [51264/60000]\n",
            "loss: 0.407650  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.446023 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.266395  [   64/60000]\n",
            "loss: 0.444113  [ 6464/60000]\n",
            "loss: 0.271035  [12864/60000]\n",
            "loss: 0.447134  [19264/60000]\n",
            "loss: 0.348718  [25664/60000]\n",
            "loss: 0.423805  [32064/60000]\n",
            "loss: 0.421450  [38464/60000]\n",
            "loss: 0.597546  [44864/60000]\n",
            "loss: 0.495753  [51264/60000]\n",
            "loss: 0.406953  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.444937 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.265042  [   64/60000]\n",
            "loss: 0.442137  [ 6464/60000]\n",
            "loss: 0.269916  [12864/60000]\n",
            "loss: 0.445874  [19264/60000]\n",
            "loss: 0.346490  [25664/60000]\n",
            "loss: 0.422768  [32064/60000]\n",
            "loss: 0.420045  [38464/60000]\n",
            "loss: 0.596046  [44864/60000]\n",
            "loss: 0.494021  [51264/60000]\n",
            "loss: 0.406126  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.443861 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.263737  [   64/60000]\n",
            "loss: 0.440158  [ 6464/60000]\n",
            "loss: 0.268936  [12864/60000]\n",
            "loss: 0.444536  [19264/60000]\n",
            "loss: 0.344273  [25664/60000]\n",
            "loss: 0.421605  [32064/60000]\n",
            "loss: 0.418526  [38464/60000]\n",
            "loss: 0.594455  [44864/60000]\n",
            "loss: 0.492219  [51264/60000]\n",
            "loss: 0.405354  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.442809 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.262429  [   64/60000]\n",
            "loss: 0.438255  [ 6464/60000]\n",
            "loss: 0.268039  [12864/60000]\n",
            "loss: 0.443207  [19264/60000]\n",
            "loss: 0.342202  [25664/60000]\n",
            "loss: 0.420492  [32064/60000]\n",
            "loss: 0.417052  [38464/60000]\n",
            "loss: 0.592978  [44864/60000]\n",
            "loss: 0.490454  [51264/60000]\n",
            "loss: 0.404516  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.441758 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.261104  [   64/60000]\n",
            "loss: 0.436273  [ 6464/60000]\n",
            "loss: 0.267172  [12864/60000]\n",
            "loss: 0.441879  [19264/60000]\n",
            "loss: 0.340047  [25664/60000]\n",
            "loss: 0.419330  [32064/60000]\n",
            "loss: 0.415673  [38464/60000]\n",
            "loss: 0.591522  [44864/60000]\n",
            "loss: 0.488707  [51264/60000]\n",
            "loss: 0.403725  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.440710 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.259856  [   64/60000]\n",
            "loss: 0.434397  [ 6464/60000]\n",
            "loss: 0.266417  [12864/60000]\n",
            "loss: 0.440573  [19264/60000]\n",
            "loss: 0.337957  [25664/60000]\n",
            "loss: 0.418284  [32064/60000]\n",
            "loss: 0.414239  [38464/60000]\n",
            "loss: 0.590064  [44864/60000]\n",
            "loss: 0.487018  [51264/60000]\n",
            "loss: 0.403085  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 0.439675 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.258577  [   64/60000]\n",
            "loss: 0.432556  [ 6464/60000]\n",
            "loss: 0.265611  [12864/60000]\n",
            "loss: 0.439231  [19264/60000]\n",
            "loss: 0.335890  [25664/60000]\n",
            "loss: 0.417100  [32064/60000]\n",
            "loss: 0.412822  [38464/60000]\n",
            "loss: 0.588703  [44864/60000]\n",
            "loss: 0.485306  [51264/60000]\n",
            "loss: 0.402450  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 0.438662 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.257356  [   64/60000]\n",
            "loss: 0.430673  [ 6464/60000]\n",
            "loss: 0.264887  [12864/60000]\n",
            "loss: 0.437817  [19264/60000]\n",
            "loss: 0.333815  [25664/60000]\n",
            "loss: 0.415874  [32064/60000]\n",
            "loss: 0.411359  [38464/60000]\n",
            "loss: 0.587382  [44864/60000]\n",
            "loss: 0.483570  [51264/60000]\n",
            "loss: 0.401736  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.437660 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.256069  [   64/60000]\n",
            "loss: 0.428872  [ 6464/60000]\n",
            "loss: 0.264130  [12864/60000]\n",
            "loss: 0.436486  [19264/60000]\n",
            "loss: 0.331799  [25664/60000]\n",
            "loss: 0.414700  [32064/60000]\n",
            "loss: 0.409853  [38464/60000]\n",
            "loss: 0.585982  [44864/60000]\n",
            "loss: 0.481830  [51264/60000]\n",
            "loss: 0.401001  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.436676 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.254863  [   64/60000]\n",
            "loss: 0.427069  [ 6464/60000]\n",
            "loss: 0.263349  [12864/60000]\n",
            "loss: 0.435194  [19264/60000]\n",
            "loss: 0.329826  [25664/60000]\n",
            "loss: 0.413499  [32064/60000]\n",
            "loss: 0.408495  [38464/60000]\n",
            "loss: 0.584678  [44864/60000]\n",
            "loss: 0.480168  [51264/60000]\n",
            "loss: 0.400306  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.435705 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.253730  [   64/60000]\n",
            "loss: 0.425232  [ 6464/60000]\n",
            "loss: 0.262638  [12864/60000]\n",
            "loss: 0.433846  [19264/60000]\n",
            "loss: 0.328003  [25664/60000]\n",
            "loss: 0.412298  [32064/60000]\n",
            "loss: 0.407114  [38464/60000]\n",
            "loss: 0.583341  [44864/60000]\n",
            "loss: 0.478371  [51264/60000]\n",
            "loss: 0.399561  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.434760 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.252619  [   64/60000]\n",
            "loss: 0.423428  [ 6464/60000]\n",
            "loss: 0.261936  [12864/60000]\n",
            "loss: 0.432527  [19264/60000]\n",
            "loss: 0.326204  [25664/60000]\n",
            "loss: 0.411100  [32064/60000]\n",
            "loss: 0.405746  [38464/60000]\n",
            "loss: 0.582009  [44864/60000]\n",
            "loss: 0.476701  [51264/60000]\n",
            "loss: 0.398875  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.433815 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGOqHO_2O7ai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}